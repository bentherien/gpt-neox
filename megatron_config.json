{"launcher": "jsrun", "train_batch_size": 1104, "train_micro_batch_size_per_gpu": 4, "optimizer": {"type": "Adam", "params": {"lr": 0.0003, "betas": [0.9, 0.95], "eps": 1e-08}}, "fp16": {"enabled": true, "loss_scale": 0, "loss_scale_window": 1000, "hysteresis": 2, "min_loss_scale": 1}, "gradient_clipping": 1.0, "zero_optimization": {"stage": 1, "allgather_partitions": true, "allgather_bucket_size": 500000000, "overlap_comm": true, "reduce_scatter": true, "reduce_bucket_size": 500000000, "contiguous_gradients": true, "cpu_offload": false}, "steps_per_print": 1, "wall_clock_breakdown": true, "precision": "fp16", "num_layers": 24, "hidden_size": 1024, "num_attention_heads": 16, "seq_length": 2048, "max_position_embeddings": 2048, "pos_emb": "rotary", "no_weight_tying": true, "attention_config": ["global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global"], "sparsity_config": {}, "scaled_upper_triang_masked_softmax_fusion": true, "bias_gelu_fusion": true, "rotary_pct": 0.25, "init_method": "small_init", "output_layer_init_method": "wang_init", "gpt_j_residual": true, "output_layer_parallelism": "column", "identifier_string": "pythia-c-410M", "lr_decay_style": "cosine", "lr_decay_iters": 181793, "min_lr": 0.0003, "optimizer_type": "Adam", "zero_stage": 1, "zero_reduce_scatter": true, "zero_contiguous_gradients": true, "zero_reduce_bucket_size": 500000000, "zero_allgather_bucket_size": 500000000, "lr": 0.0003, "tokenizer_type": "HFTokenizer", "train_data_paths": ["/gpfs/alpine/csc499/scratch/btherien/gpt-neox/data/red_pajama_400B/arxiv/folder_train/tokenized_text_document", "/gpfs/alpine/csc499/scratch/btherien/gpt-neox/data/red_pajama_400B/book/folder_train/tokenized_text_document", "/gpfs/alpine/csc499/scratch/btherien/gpt-neox/data/red_pajama_400B/c4/folder_train/tokenized_text_document", "/gpfs/alpine/csc499/scratch/btherien/gpt-neox/data/red_pajama_400B/wikipedia/folder_train/tokenized_text_document", "/gpfs/alpine/csc499/scratch/btherien/gpt-neox/data/red_pajama_400B/github/folder_train/tokenized_text_document", "/gpfs/alpine/csc499/scratch/btherien/gpt-neox/data/red_pajama_400B/common_crawl/2019-30/folder_train/tokenized_text_document", "/gpfs/alpine/csc499/scratch/btherien/gpt-neox/data/red_pajama_400B/common_crawl/2020-05/folder_train/tokenized_text_document", "/gpfs/alpine/csc499/scratch/btherien/gpt-neox/data/red_pajama_400B/common_crawl/2021-04/folder_train/tokenized_text_document", "/gpfs/alpine/csc499/scratch/btherien/gpt-neox/data/red_pajama_400B/common_crawl/2022-05/folder_train/tokenized_text_document", "/gpfs/alpine/csc499/scratch/btherien/gpt-neox/data/red_pajama_400B/common_crawl/2023-06/folder_train/tokenized_text_document"], "test_data_paths": ["/gpfs/alpine/csc499/scratch/btherien/gpt-neox/data/red_pajama_400B/the_pile/test_tokenized_text_document"], "valid_data_paths": [["/gpfs/alpine/csc499/scratch/btherien/gpt-neox/data/red_pajama_400B/the_pile/val_tokenized_text_document"], ["/gpfs/alpine/csc499/scratch/btherien/gpt-neox/data/red_pajama_400B/arxiv/folder_val/tokenized_text_document", "/gpfs/alpine/csc499/scratch/btherien/gpt-neox/data/red_pajama_400B/book/folder_val/tokenized_text_document", "/gpfs/alpine/csc499/scratch/btherien/gpt-neox/data/red_pajama_400B/c4/folder_val/tokenized_text_document", "/gpfs/alpine/csc499/scratch/btherien/gpt-neox/data/red_pajama_400B/wikipedia/folder_val/tokenized_text_document", "/gpfs/alpine/csc499/scratch/btherien/gpt-neox/data/red_pajama_400B/github/folder_val/tokenized_text_document", "/gpfs/alpine/csc499/scratch/btherien/gpt-neox/data/red_pajama_400B/common_crawl/2019-30/folder_val/tokenized_text_document", "/gpfs/alpine/csc499/scratch/btherien/gpt-neox/data/red_pajama_400B/common_crawl/2020-05/folder_val/tokenized_text_document", "/gpfs/alpine/csc499/scratch/btherien/gpt-neox/data/red_pajama_400B/common_crawl/2021-04/folder_val/tokenized_text_document", "/gpfs/alpine/csc499/scratch/btherien/gpt-neox/data/red_pajama_400B/common_crawl/2022-05/folder_val/tokenized_text_document", "/gpfs/alpine/csc499/scratch/btherien/gpt-neox/data/red_pajama_400B/common_crawl/2023-06/folder_val/tokenized_text_document"]], "train_data_weights": [2.5, 4.5, 15.0, 4.5, 4.5, 13.4, 13.4, 13.4, 13.4, 13.4], "valid_data_weights": [[1.0], [2.5, 4.5, 15.0, 4.5, 4.5, 13.4, 13.4, 13.4, 13.4, 13.4]], "test_data_weights": [1.0], "data_impl": "mmap", "save": "/gpfs/alpine/csc499/scratch/btherien/gpt-neox/checkpoints/continued_test8", "config_files": {"410M.yml": "# GPT-2 pretraining setup\n{\n  # parallelism settings ( you will want to change these based on your cluster setup, ideally scheduling pipeline stages\n  # across the node boundaries )\n  \"pipe-parallel-size\": 1,\n  \"model-parallel-size\": 1, # one copy of the model per node\n\n  # model settings\n  \"num-layers\": 24,\n  \"hidden-size\": 1024,\n  \"num-attention-heads\": 16,\n  \"seq-length\": 2048,\n  \"max-position-embeddings\": 2048,\n  \"pos-emb\": \"rotary\",\n  \"rotary-pct\": 0.25,\n  \"no-weight-tying\": true,\n  \"gpt-j-residual\": true,\n  \"output-layer-parallelism\": \"column\",\n\n  # these should provide some speedup but takes a while to build, set to true if desired\n  \"scaled-upper-triang-masked-softmax-fusion\": true,\n  \"bias-gelu-fusion\": true,\n\n  # init methods\n  \"init_method\": \"small_init\",\n  \"output_layer_init_method\": \"wang_init\",\n\n  # \"optimizer\": {\n  #   \"type\": \"Adam\",\n  #   \"params\": {\n  #     \"lr\": 3.0e-4,\n  #     \"betas\": [0.9, 0.95],\n  #     \"eps\": 1.0e-8,\n  #   }\n  # },\n  # \"min_lr\": 3.0e-5,\n\n  \"zero_optimization\": {\n    \"stage\": 1,\n    \"allgather_partitions\": True,\n    \"allgather_bucket_size\": 500000000,\n    \"overlap_comm\": True,\n    \"reduce_scatter\": True,\n    \"reduce_bucket_size\": 500000000,\n    \"contiguous_gradients\": True,\n    \"cpu_offload\": False\n  },\n\n  # LLAMA Config\n  # batch / data settings\n  \"train_batch_size\": 1104, #1104, #1104 # approximately 2.2M batch size across 46 nodes \n  \"train_micro_batch_size_per_gpu\": 4,\n  \"data-impl\": \"mmap\",\n  \"split\": \"949,50,1\",\n\n  # activation checkpointing\n  \"checkpoint-activations\": true,\n  \"checkpoint-num-layers\": 1,\n  \"partition-activations\": true,\n  \"synchronize-each-layer\": true,\n\n  # regularization\n  \"gradient_clipping\": 1.0,\n  \"weight-decay\": 0.1,\n  \"hidden-dropout\": 0.0,\n  \"attention-dropout\": 0.0,\n\n  # precision settings of LLaMa\n  \"fp16\": {\n    \"enabled\": true,\n    # \"type\": \"bfloat16\", # set bf16 as precision\n    \"loss_scale\": 0,\n    \"loss_scale_window\": 1000,\n    \"hysteresis\": 2,\n    \"min_loss_scale\": 1\n  },\n\n  # \"fp32_allreduce\": True, # without a patch to torch, bf16 models have to do the allreduce in fp32\n  # misc. training settings\n  \"train-iters\": 181793,\n  \"lr-decay-iters\": 181793,\n  \"distributed-backend\": \"nccl\",\n  # \"lr-decay-style\": \"cosine\",\n  # \"warmup\": 0.01,\n  \"checkpoint-factor\": 1000,\n  \"eval-interval\": 100,\n  \"warup-eval-interval\": 50,\n  \"eval-iters\": 10,\n\n  # logging\n  \"log-interval\": 1,\n  \"steps_per_print\": 1,\n  \"keep-last-n-checkpoints\": 1000,\n  \"wall_clock_breakdown\": true,\n\n  \"identifier_string\": \"pythia-c-410M\"\n}", "pythia_llama_setup_ben.yml": "# Suggested data paths when using GPT-NeoX locally\n{\n  # \"data-path\": \"data/enwik8/enwik8_text_document\",\n\n  # or for weighted datasets:\n  \"train-data-paths\": [\n    \"/gpfs/alpine/csc499/scratch/btherien/gpt-neox/data/red_pajama_400B/arxiv/folder_train/tokenized_text_document\",\n    \"/gpfs/alpine/csc499/scratch/btherien/gpt-neox/data/red_pajama_400B/book/folder_train/tokenized_text_document\",\n    \"/gpfs/alpine/csc499/scratch/btherien/gpt-neox/data/red_pajama_400B/c4/folder_train/tokenized_text_document\",\n    \"/gpfs/alpine/csc499/scratch/btherien/gpt-neox/data/red_pajama_400B/wikipedia/folder_train/tokenized_text_document\",\n    \"/gpfs/alpine/csc499/scratch/btherien/gpt-neox/data/red_pajama_400B/github/folder_train/tokenized_text_document\",\n    \"/gpfs/alpine/csc499/scratch/btherien/gpt-neox/data/red_pajama_400B/common_crawl/2019-30/folder_train/tokenized_text_document\",\n    \"/gpfs/alpine/csc499/scratch/btherien/gpt-neox/data/red_pajama_400B/common_crawl/2020-05/folder_train/tokenized_text_document\",\n    \"/gpfs/alpine/csc499/scratch/btherien/gpt-neox/data/red_pajama_400B/common_crawl/2021-04/folder_train/tokenized_text_document\",\n    \"/gpfs/alpine/csc499/scratch/btherien/gpt-neox/data/red_pajama_400B/common_crawl/2022-05/folder_train/tokenized_text_document\",\n    \"/gpfs/alpine/csc499/scratch/btherien/gpt-neox/data/red_pajama_400B/common_crawl/2023-06/folder_train/tokenized_text_document\",\n  ],\n  \"test-data-paths\": [\"/gpfs/alpine/csc499/scratch/btherien/gpt-neox/data/red_pajama_400B/the_pile/test_tokenized_text_document\"],\n  \"valid-data-paths\": [\n    [\"/gpfs/alpine/csc499/scratch/btherien/gpt-neox/data/red_pajama_400B/the_pile/val_tokenized_text_document\"],\n    [\n    \"/gpfs/alpine/csc499/scratch/btherien/gpt-neox/data/red_pajama_400B/arxiv/folder_val/tokenized_text_document\",\n    \"/gpfs/alpine/csc499/scratch/btherien/gpt-neox/data/red_pajama_400B/book/folder_val/tokenized_text_document\",\n    \"/gpfs/alpine/csc499/scratch/btherien/gpt-neox/data/red_pajama_400B/c4/folder_val/tokenized_text_document\",\n    \"/gpfs/alpine/csc499/scratch/btherien/gpt-neox/data/red_pajama_400B/wikipedia/folder_val/tokenized_text_document\",\n    \"/gpfs/alpine/csc499/scratch/btherien/gpt-neox/data/red_pajama_400B/github/folder_val/tokenized_text_document\",\n    \"/gpfs/alpine/csc499/scratch/btherien/gpt-neox/data/red_pajama_400B/common_crawl/2019-30/folder_val/tokenized_text_document\",\n    \"/gpfs/alpine/csc499/scratch/btherien/gpt-neox/data/red_pajama_400B/common_crawl/2020-05/folder_val/tokenized_text_document\",\n    \"/gpfs/alpine/csc499/scratch/btherien/gpt-neox/data/red_pajama_400B/common_crawl/2021-04/folder_val/tokenized_text_document\",\n    \"/gpfs/alpine/csc499/scratch/btherien/gpt-neox/data/red_pajama_400B/common_crawl/2022-05/folder_val/tokenized_text_document\",\n    \"/gpfs/alpine/csc499/scratch/btherien/gpt-neox/data/red_pajama_400B/common_crawl/2023-06/folder_val/tokenized_text_document\",\n    ],\n    ],\n  \"train-data-weights\": [\n    2.5,\n    4.5,\n    15.0,\n    4.5,\n    4.5,\n    13.4,\n    13.4,\n    13.4,\n    13.4,\n    13.4\n  ],\n  \"test-data-weights\": [\n    1.\n  ],\n  \"valid-data-weights\": [\n    [1.],\n    [\n    2.5,\n    4.5,\n    15.0,\n    4.5,\n    4.5,\n    13.4,\n    13.4,\n    13.4,\n    13.4,\n    13.4\n    ],\n  ],\n\n  # If weight_by_num_documents is True, Builds dataset weights from a multinomial distribution over groups of data according to the number of documents in each group.\n  # WARNING: setting this to True will override any user provided weights\n  # \"weight_by_num_documents\": false,\n  # \"weighted_sampler_alpha\": 0.3,\n\n  \"tokenizer-type\": \"HFTokenizer\",\n  \"vocab-file\": \"/gpfs/alpine/csc499/scratch/btherien/gpt-neox/data/20B_tokenizer.json\",\n  # \"merge-file\": \"data/gpt2-merges.txt\",\n\n  \"save\": \"/gpfs/alpine/csc499/scratch/btherien/gpt-neox/checkpoints/continued_test8\",\n  \"load\": \"/gpfs/alpine/csc499/scratch/btherien/neox_converted/mp1_pp1/pythia\",\n\n  \"checkpoint_validation_with_forward_pass\": False,\n\n  \"tensorboard-dir\": \"tensorboard/continued_test8\",\n  \"log-dir\": \"/gpfs/alpine/csc499/scratch/btherien/gpt-neox/logs/continued_test8\",\n  \"use_wandb\": False,\n  # \"wandb_host\": \"https://api.wandb.ai\",\n  \"wandb_project\": \"continued_test8\",\n\n  \"launcher\": \"jsrun\",\n  \"deepspeed_jsrun\": true,\n  \"finetune\": true,\n  \"num_workers\": 0,\n}", "adam_cosine_lr3e-4_3e-5_wu-001.yml": "\n{\n\n\"optimizer\": {\n    \"type\": \"Adam\",\n    \"params\": {\n      \"lr\": 3.0e-4,\n      \"betas\": [0.9, 0.95],\n      \"eps\": 1.0e-8,\n    }\n  },\n  \"min_lr\": 3.0e-4,\n\n  \"lr-decay-style\": \"cosine\",\n  \"warmup\": 0.01,\n\n\n}"}, "load": "/gpfs/alpine/csc499/scratch/btherien/neox_converted/mp1_pp1/pythia", "checkpoint_factor": 1000, "finetune": true, "batch_size": 4, "train_iters": 181793, "eval_iters": 10, "keep_last_n_checkpoints": 1000, "eval_interval": 100, "split": "949,50,1", "vocab_file": "/gpfs/alpine/csc499/scratch/btherien/gpt-neox/data/20B_tokenizer.json", "num_workers": 0, "attention_dropout": 0.0, "hidden_dropout": 0.0, "weight_decay": 0.1, "checkpoint_activations": true, "synchronize_each_layer": true, "partition_activations": true, "gas": 1, "clip_grad": 1.0, "dynamic_loss_scale": true, "pipe_parallel_size": 1, "world_size": 1, "use_wandb": false, "wandb_project": "continued_test8", "log_dir": "/gpfs/alpine/csc499/scratch/btherien/gpt-neox/logs/continued_test8", "tensorboard_dir": "tensorboard/continued_test8", "log_interval": 1, "text_gen_type": "unconditional", "local_rank": 0, "rank": 0, "deepspeed_jsrun": true, "user_script": "/gpfs/alpine/csc499/scratch/btherien/gpt-neox/train.py", "save_iters": [1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000, 11000, 12000, 13000, 14000, 15000, 16000, 17000, 18000, 19000, 20000, 21000, 22000, 23000, 24000, 25000, 26000, 27000, 28000, 29000, 30000, 31000, 32000, 33000, 34000, 35000, 36000, 37000, 38000, 39000, 40000, 41000, 42000, 43000, 44000, 45000, 46000, 47000, 48000, 49000, 50000, 51000, 52000, 53000, 54000, 55000, 56000, 57000, 58000, 59000, 60000, 61000, 62000, 63000, 64000, 65000, 66000, 67000, 68000, 69000, 70000, 71000, 72000, 73000, 74000, 75000, 76000, 77000, 78000, 79000, 80000, 81000, 82000, 83000, 84000, 85000, 86000, 87000, 88000, 89000, 90000, 91000, 92000, 93000, 94000, 95000, 96000, 97000, 98000, 99000, 100000, 101000, 102000, 103000, 104000, 105000, 106000, 107000, 108000, 109000, 110000, 111000, 112000, 113000, 114000, 115000, 116000, 117000, 118000, 119000, 120000, 121000, 122000, 123000, 124000, 125000, 126000, 127000, 128000, 129000, 130000, 131000, 132000, 133000, 134000, 135000, 136000, 137000, 138000, 139000, 140000, 141000, 142000, 143000, 144000, 145000, 146000, 147000, 148000, 149000, 150000, 151000, 152000, 153000, 154000, 155000, 156000, 157000, 158000, 159000, 160000, 161000, 162000, 163000, 164000, 165000, 166000, 167000, 168000, 169000, 170000, 171000, 172000, 173000, 174000, 175000, 176000, 177000, 178000, 179000, 180000, 181000], "global_num_gpus": 276}